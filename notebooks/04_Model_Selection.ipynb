{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Master in Data Science - Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Frida Ibarra y Gema Romero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notebook 4: Module Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step is to develop a machine learning model capable of accurately predicting the probability of a customer defaulting on a loan, considering the inherent class imbalance in this type of problem.\n",
    "\n",
    "Data Balancing:\n",
    "- Analysis: Assess the degree of imbalance between the classes (paying customers vs. defaulting customers).\n",
    "- Treatment: Apply oversampling (SMOTE) or undersampling techniques to balance the dataset and mitigate bias in the model.\n",
    "\n",
    "Model Selection:\n",
    "- Exploration: Experiment with a variety of algorithms, including logistic regression, decision trees, random forest, and gradient boosting.\n",
    "- Comparison: Evaluate the performance of each model using relevant metrics such as accuracy, recall, F1-score, and AUC-ROC.\n",
    "\n",
    "Cross-Validation:\n",
    "- Robustness: Employ cross-validation to ensure the model generalizes well to new data and prevent overfitting.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "- Optimization: Utilize hyperparameter tuning techniques (grid search, random search) to find the optimal configuration for each model.\n",
    "\n",
    "Model Selection:\n",
    "- Evaluation: Compare the trained models and select the one that achieves the best results in terms of metrics and generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, ShuffleSplit\n",
    "from functools import partial\n",
    "from sklearn.model_selection import RandomizedSearchCV, ShuffleSplit\n",
    "from sklearn.metrics import make_scorer, fbeta_score, classification_report\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from joblib import dump\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src') \n",
    "import functions_src as fa  \n",
    "sys.path.remove('../src')\n",
    "\n",
    "#Seed\n",
    "seed = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE_Cash loans</th>\n",
       "      <th>NAME_CONTRACT_TYPE_Revolving loans</th>\n",
       "      <th>CODE_GENDER_F</th>\n",
       "      <th>CODE_GENDER_M</th>\n",
       "      <th>CODE_GENDER_XNA</th>\n",
       "      <th>FLAG_OWN_CAR_N</th>\n",
       "      <th>FLAG_OWN_CAR_Y</th>\n",
       "      <th>FLAG_OWN_REALTY_N</th>\n",
       "      <th>FLAG_OWN_REALTY_Y</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE_missing</th>\n",
       "      <th>NAME_TYPE_SUITE_Group of people</th>\n",
       "      <th>NAME_INCOME_TYPE_Working</th>\n",
       "      <th>NAME_INCOME_TYPE_Commercial associate</th>\n",
       "      <th>NAME_INCOME_TYPE_State servant</th>\n",
       "      <th>NAME_INCOME_TYPE_Maternity leave</th>\n",
       "      <th>NAME_INCOME_TYPE_Businessman</th>\n",
       "      <th>NAME_INCOME_TYPE_Unemployed</th>\n",
       "      <th>NAME_FAMILY_STATUS_Married</th>\n",
       "      <th>NAME_FAMILY_STATUS_Civil marriage</th>\n",
       "      <th>NAME_FAMILY_STATUS_Widow</th>\n",
       "      <th>NAME_FAMILY_STATUS_Separated</th>\n",
       "      <th>NAME_FAMILY_STATUS_Single / not married</th>\n",
       "      <th>NAME_HOUSING_TYPE_House / apartment</th>\n",
       "      <th>NAME_HOUSING_TYPE_Municipal apartment</th>\n",
       "      <th>NAME_HOUSING_TYPE_With parents</th>\n",
       "      <th>NAME_HOUSING_TYPE_Rented apartment</th>\n",
       "      <th>NAME_HOUSING_TYPE_Office apartment</th>\n",
       "      <th>NAME_HOUSING_TYPE_Co-op apartment</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <th>ELEVATORS_AVG</th>\n",
       "      <th>ENTRANCES_AVG</th>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <th>LANDAREA_AVG</th>\n",
       "      <th>LIVINGAPARTMENTS_AVG</th>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <th>NONLIVINGAREA_AVG</th>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <th>ELEVATORS_MODE</th>\n",
       "      <th>ENTRANCES_MODE</th>\n",
       "      <th>FLOORSMIN_MODE</th>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <th>NONLIVINGAREA_MODE</th>\n",
       "      <th>YEARS_BUILD_MEDI</th>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <th>ELEVATORS_MEDI</th>\n",
       "      <th>ENTRANCES_MEDI</th>\n",
       "      <th>FLOORSMIN_MEDI</th>\n",
       "      <th>LANDAREA_MEDI</th>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <th>FONDKAPREMONT_MODE_missing</th>\n",
       "      <th>FONDKAPREMONT_MODE_reg oper spec account</th>\n",
       "      <th>FONDKAPREMONT_MODE_reg oper account</th>\n",
       "      <th>FONDKAPREMONT_MODE_not specified</th>\n",
       "      <th>FONDKAPREMONT_MODE_org spec account</th>\n",
       "      <th>HOUSETYPE_MODE_missing</th>\n",
       "      <th>HOUSETYPE_MODE_block of flats</th>\n",
       "      <th>HOUSETYPE_MODE_terraced house</th>\n",
       "      <th>HOUSETYPE_MODE_specific housing</th>\n",
       "      <th>WALLSMATERIAL_MODE_missing</th>\n",
       "      <th>WALLSMATERIAL_MODE_Monolithic</th>\n",
       "      <th>WALLSMATERIAL_MODE_Others</th>\n",
       "      <th>EMERGENCYSTATE_MODE_missing</th>\n",
       "      <th>EMERGENCYSTATE_MODE_No</th>\n",
       "      <th>EMERGENCYSTATE_MODE_Yes</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>FLAG_DOCUMENT_8</th>\n",
       "      <th>FLAG_DOCUMENT_11</th>\n",
       "      <th>FLAG_DOCUMENT_16</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>NAME_EDUCATION_TYPE_ORDINAL</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START_ORDINAL</th>\n",
       "      <th>OCCUPATION_TYPE_MEAN</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.786989</td>\n",
       "      <td>0.324103</td>\n",
       "      <td>-0.324103</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>-0.722031</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>-0.716969</td>\n",
       "      <td>1.503079</td>\n",
       "      <td>-1.503079</td>\n",
       "      <td>0.807356</td>\n",
       "      <td>-0.320471</td>\n",
       "      <td>0.654397</td>\n",
       "      <td>-0.120991</td>\n",
       "      <td>0.491406</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>-0.029781</td>\n",
       "      <td>0.967741</td>\n",
       "      <td>-0.550989</td>\n",
       "      <td>-0.275698</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.752689</td>\n",
       "      <td>-0.327059</td>\n",
       "      <td>-0.235098</td>\n",
       "      <td>-0.262467</td>\n",
       "      <td>-0.417105</td>\n",
       "      <td>0.356817</td>\n",
       "      <td>-0.194440</td>\n",
       "      <td>-0.225169</td>\n",
       "      <td>-0.127727</td>\n",
       "      <td>-0.093034</td>\n",
       "      <td>-0.060291</td>\n",
       "      <td>0.968781</td>\n",
       "      <td>-0.477982</td>\n",
       "      <td>-0.146275</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.468486</td>\n",
       "      <td>-0.499666</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>-0.625627</td>\n",
       "      <td>-0.245406</td>\n",
       "      <td>0.930833</td>\n",
       "      <td>1.860366</td>\n",
       "      <td>1.925206</td>\n",
       "      <td>-1.246885</td>\n",
       "      <td>-0.206529</td>\n",
       "      <td>2.136860</td>\n",
       "      <td>-0.674873</td>\n",
       "      <td>-2.822123</td>\n",
       "      <td>-2.290413</td>\n",
       "      <td>-0.771477</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>-0.165351</td>\n",
       "      <td>-0.368302</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.140632</td>\n",
       "      <td>-0.149356</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>-0.230110</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>-0.168474</td>\n",
       "      <td>-0.355921</td>\n",
       "      <td>-0.050833</td>\n",
       "      <td>-0.070287</td>\n",
       "      <td>-0.159456</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>-0.237963</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>-0.166514</td>\n",
       "      <td>-0.365151</td>\n",
       "      <td>-0.078828</td>\n",
       "      <td>-0.082291</td>\n",
       "      <td>-0.141441</td>\n",
       "      <td>-0.151557</td>\n",
       "      <td>-0.098784</td>\n",
       "      <td>-0.231454</td>\n",
       "      <td>0.678940</td>\n",
       "      <td>-0.201525</td>\n",
       "      <td>-0.561708</td>\n",
       "      <td>-0.136934</td>\n",
       "      <td>-0.136317</td>\n",
       "      <td>0.995603</td>\n",
       "      <td>-0.978230</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>-0.069778</td>\n",
       "      <td>0.982008</td>\n",
       "      <td>-0.075816</td>\n",
       "      <td>-0.072802</td>\n",
       "      <td>1.052419</td>\n",
       "      <td>-1.036392</td>\n",
       "      <td>-0.087849</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>-0.297606</td>\n",
       "      <td>-0.061999</td>\n",
       "      <td>-0.100151</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>-0.070444</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.270689</td>\n",
       "      <td>2.269190</td>\n",
       "      <td>1.827981</td>\n",
       "      <td>-0.582747</td>\n",
       "      <td>-0.295316</td>\n",
       "      <td>-0.659055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.474027</td>\n",
       "      <td>0.324103</td>\n",
       "      <td>-0.324103</td>\n",
       "      <td>-1.384932</td>\n",
       "      <td>1.384982</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>-1.394761</td>\n",
       "      <td>1.394761</td>\n",
       "      <td>-0.665301</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>-0.577365</td>\n",
       "      <td>0.041994</td>\n",
       "      <td>1.012907</td>\n",
       "      <td>1.080520</td>\n",
       "      <td>0.978511</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>-0.029781</td>\n",
       "      <td>0.967741</td>\n",
       "      <td>-0.550989</td>\n",
       "      <td>-0.275698</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.752689</td>\n",
       "      <td>-0.327059</td>\n",
       "      <td>-0.235098</td>\n",
       "      <td>-0.262467</td>\n",
       "      <td>-0.417105</td>\n",
       "      <td>0.356817</td>\n",
       "      <td>-0.194440</td>\n",
       "      <td>-0.225169</td>\n",
       "      <td>-0.127727</td>\n",
       "      <td>-0.093034</td>\n",
       "      <td>-0.060291</td>\n",
       "      <td>0.495804</td>\n",
       "      <td>-0.475964</td>\n",
       "      <td>0.558202</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.468486</td>\n",
       "      <td>-0.499666</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>-0.625627</td>\n",
       "      <td>-0.245406</td>\n",
       "      <td>-0.166762</td>\n",
       "      <td>1.860366</td>\n",
       "      <td>1.925206</td>\n",
       "      <td>0.898051</td>\n",
       "      <td>-0.206529</td>\n",
       "      <td>-0.467976</td>\n",
       "      <td>0.643345</td>\n",
       "      <td>-1.676714</td>\n",
       "      <td>-0.205557</td>\n",
       "      <td>1.056196</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>-0.165351</td>\n",
       "      <td>-0.368302</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.140632</td>\n",
       "      <td>-0.149356</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>-0.230110</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>-0.168474</td>\n",
       "      <td>-0.355921</td>\n",
       "      <td>-0.050833</td>\n",
       "      <td>-0.070287</td>\n",
       "      <td>-0.159456</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>-0.237963</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>-0.166514</td>\n",
       "      <td>-0.365151</td>\n",
       "      <td>-0.078828</td>\n",
       "      <td>-0.082291</td>\n",
       "      <td>-0.141441</td>\n",
       "      <td>-0.151557</td>\n",
       "      <td>-0.098784</td>\n",
       "      <td>-0.231454</td>\n",
       "      <td>0.678940</td>\n",
       "      <td>-0.201525</td>\n",
       "      <td>-0.561708</td>\n",
       "      <td>-0.136934</td>\n",
       "      <td>-0.136317</td>\n",
       "      <td>0.995603</td>\n",
       "      <td>-0.978230</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>-0.069778</td>\n",
       "      <td>0.982008</td>\n",
       "      <td>-0.075816</td>\n",
       "      <td>-0.072802</td>\n",
       "      <td>1.052419</td>\n",
       "      <td>-1.036392</td>\n",
       "      <td>-0.087849</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>-0.297606</td>\n",
       "      <td>-0.061999</td>\n",
       "      <td>-0.100151</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>-0.070444</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.270689</td>\n",
       "      <td>-0.294438</td>\n",
       "      <td>-0.440420</td>\n",
       "      <td>-0.582747</td>\n",
       "      <td>-0.295316</td>\n",
       "      <td>-0.722256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.833941</td>\n",
       "      <td>0.324103</td>\n",
       "      <td>-0.324103</td>\n",
       "      <td>-1.384932</td>\n",
       "      <td>1.384982</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>-0.716969</td>\n",
       "      <td>-0.665301</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>-0.577365</td>\n",
       "      <td>0.041994</td>\n",
       "      <td>-0.894437</td>\n",
       "      <td>-0.913629</td>\n",
       "      <td>-0.945555</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>-0.029781</td>\n",
       "      <td>0.967741</td>\n",
       "      <td>-0.550989</td>\n",
       "      <td>-0.275698</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.752689</td>\n",
       "      <td>-0.327059</td>\n",
       "      <td>-0.235098</td>\n",
       "      <td>-0.262467</td>\n",
       "      <td>-0.417105</td>\n",
       "      <td>0.356817</td>\n",
       "      <td>-0.194440</td>\n",
       "      <td>-0.225169</td>\n",
       "      <td>-0.127727</td>\n",
       "      <td>-0.093034</td>\n",
       "      <td>-0.060291</td>\n",
       "      <td>-0.103667</td>\n",
       "      <td>-0.512889</td>\n",
       "      <td>-0.146275</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.468486</td>\n",
       "      <td>-0.499666</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>-0.625627</td>\n",
       "      <td>-0.245406</td>\n",
       "      <td>-0.166762</td>\n",
       "      <td>-0.103735</td>\n",
       "      <td>-0.063215</td>\n",
       "      <td>-0.021207</td>\n",
       "      <td>-0.206529</td>\n",
       "      <td>-0.467976</td>\n",
       "      <td>-0.411544</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>1.165691</td>\n",
       "      <td>0.604363</td>\n",
       "      <td>-2.887678</td>\n",
       "      <td>-0.483083</td>\n",
       "      <td>-0.368302</td>\n",
       "      <td>-1.054522</td>\n",
       "      <td>-1.894904</td>\n",
       "      <td>-0.675604</td>\n",
       "      <td>-1.375431</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>-0.304587</td>\n",
       "      <td>-2.839846</td>\n",
       "      <td>-0.443733</td>\n",
       "      <td>-0.355921</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-1.886891</td>\n",
       "      <td>-1.324711</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>-0.260444</td>\n",
       "      <td>-2.880656</td>\n",
       "      <td>-0.477606</td>\n",
       "      <td>-0.365151</td>\n",
       "      <td>-1.048141</td>\n",
       "      <td>-1.887217</td>\n",
       "      <td>-0.671047</td>\n",
       "      <td>-1.369013</td>\n",
       "      <td>-0.098784</td>\n",
       "      <td>-0.294935</td>\n",
       "      <td>-1.472885</td>\n",
       "      <td>4.962173</td>\n",
       "      <td>-0.561708</td>\n",
       "      <td>-0.136934</td>\n",
       "      <td>-0.136317</td>\n",
       "      <td>-1.004416</td>\n",
       "      <td>1.022255</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>-0.069778</td>\n",
       "      <td>-1.018322</td>\n",
       "      <td>-0.075816</td>\n",
       "      <td>-0.072802</td>\n",
       "      <td>-0.950192</td>\n",
       "      <td>0.964886</td>\n",
       "      <td>-0.087849</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>-0.297606</td>\n",
       "      <td>-0.061999</td>\n",
       "      <td>-0.100151</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>-0.070444</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>4.422838</td>\n",
       "      <td>-0.294438</td>\n",
       "      <td>1.260881</td>\n",
       "      <td>-0.582747</td>\n",
       "      <td>1.938114</td>\n",
       "      <td>-0.722256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.475747</td>\n",
       "      <td>-3.085434</td>\n",
       "      <td>3.085434</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>-0.722031</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>-0.716969</td>\n",
       "      <td>-0.665301</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>0.807356</td>\n",
       "      <td>-0.337731</td>\n",
       "      <td>-1.040702</td>\n",
       "      <td>-1.247176</td>\n",
       "      <td>-0.969910</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>-0.029781</td>\n",
       "      <td>0.967741</td>\n",
       "      <td>-0.550989</td>\n",
       "      <td>-0.275698</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.752689</td>\n",
       "      <td>-0.327059</td>\n",
       "      <td>-0.235098</td>\n",
       "      <td>-0.262467</td>\n",
       "      <td>-0.417105</td>\n",
       "      <td>0.356817</td>\n",
       "      <td>-0.194440</td>\n",
       "      <td>-0.225169</td>\n",
       "      <td>-0.127727</td>\n",
       "      <td>-0.093034</td>\n",
       "      <td>-0.060291</td>\n",
       "      <td>1.746994</td>\n",
       "      <td>-0.461860</td>\n",
       "      <td>-0.146275</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.468486</td>\n",
       "      <td>-0.499666</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>-0.625627</td>\n",
       "      <td>-0.245406</td>\n",
       "      <td>0.930833</td>\n",
       "      <td>-0.103735</td>\n",
       "      <td>-0.063215</td>\n",
       "      <td>0.591632</td>\n",
       "      <td>-0.206529</td>\n",
       "      <td>-0.467976</td>\n",
       "      <td>4.344386</td>\n",
       "      <td>-2.064219</td>\n",
       "      <td>0.299219</td>\n",
       "      <td>0.112514</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>-0.165351</td>\n",
       "      <td>-0.368302</td>\n",
       "      <td>-1.541198</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.140632</td>\n",
       "      <td>-0.149356</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>-0.304587</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>-0.168474</td>\n",
       "      <td>-0.355921</td>\n",
       "      <td>-1.499623</td>\n",
       "      <td>-0.070287</td>\n",
       "      <td>-0.159456</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>-0.260444</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>-0.166514</td>\n",
       "      <td>-0.365151</td>\n",
       "      <td>-1.533500</td>\n",
       "      <td>-0.082291</td>\n",
       "      <td>-0.141441</td>\n",
       "      <td>-0.151557</td>\n",
       "      <td>-0.098784</td>\n",
       "      <td>-0.294935</td>\n",
       "      <td>0.678940</td>\n",
       "      <td>-0.201525</td>\n",
       "      <td>-0.561708</td>\n",
       "      <td>-0.136934</td>\n",
       "      <td>-0.136317</td>\n",
       "      <td>-1.004416</td>\n",
       "      <td>1.022255</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>-0.069778</td>\n",
       "      <td>-1.018322</td>\n",
       "      <td>-0.075816</td>\n",
       "      <td>-0.072802</td>\n",
       "      <td>-0.950192</td>\n",
       "      <td>0.964886</td>\n",
       "      <td>-0.087849</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>-0.297606</td>\n",
       "      <td>-0.061999</td>\n",
       "      <td>-0.100151</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>-0.070444</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.270689</td>\n",
       "      <td>-0.294438</td>\n",
       "      <td>-0.440420</td>\n",
       "      <td>-0.582747</td>\n",
       "      <td>0.263041</td>\n",
       "      <td>1.158337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.168650</td>\n",
       "      <td>0.324103</td>\n",
       "      <td>-0.324103</td>\n",
       "      <td>0.722057</td>\n",
       "      <td>-0.722031</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>-0.716969</td>\n",
       "      <td>-0.665301</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>-0.577365</td>\n",
       "      <td>-0.165129</td>\n",
       "      <td>0.188603</td>\n",
       "      <td>-0.358132</td>\n",
       "      <td>0.369630</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>-0.029781</td>\n",
       "      <td>-1.033335</td>\n",
       "      <td>-0.550989</td>\n",
       "      <td>-0.275698</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.752689</td>\n",
       "      <td>-0.327059</td>\n",
       "      <td>-0.235098</td>\n",
       "      <td>-0.262467</td>\n",
       "      <td>-0.417105</td>\n",
       "      <td>-2.802553</td>\n",
       "      <td>5.142968</td>\n",
       "      <td>-0.225169</td>\n",
       "      <td>-0.127727</td>\n",
       "      <td>-0.093034</td>\n",
       "      <td>-0.060291</td>\n",
       "      <td>-1.567512</td>\n",
       "      <td>2.134588</td>\n",
       "      <td>-0.146275</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>-2.134535</td>\n",
       "      <td>-0.499666</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>1.598398</td>\n",
       "      <td>-0.245406</td>\n",
       "      <td>-0.166762</td>\n",
       "      <td>-0.103735</td>\n",
       "      <td>-0.063215</td>\n",
       "      <td>0.285212</td>\n",
       "      <td>-0.206529</td>\n",
       "      <td>-0.467976</td>\n",
       "      <td>-1.363261</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>0.675098</td>\n",
       "      <td>-1.401479</td>\n",
       "      <td>-1.230159</td>\n",
       "      <td>-0.613434</td>\n",
       "      <td>-0.368302</td>\n",
       "      <td>-1.054522</td>\n",
       "      <td>-1.442571</td>\n",
       "      <td>-0.639431</td>\n",
       "      <td>-1.313659</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>-0.304587</td>\n",
       "      <td>-1.201732</td>\n",
       "      <td>-0.576617</td>\n",
       "      <td>-0.355921</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-1.433285</td>\n",
       "      <td>-1.259485</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>-0.260444</td>\n",
       "      <td>-1.226554</td>\n",
       "      <td>-0.607614</td>\n",
       "      <td>-0.365151</td>\n",
       "      <td>-1.048141</td>\n",
       "      <td>-1.436527</td>\n",
       "      <td>-0.637243</td>\n",
       "      <td>-1.306105</td>\n",
       "      <td>-0.098784</td>\n",
       "      <td>-0.294935</td>\n",
       "      <td>-1.472885</td>\n",
       "      <td>-0.201525</td>\n",
       "      <td>1.780285</td>\n",
       "      <td>-0.136934</td>\n",
       "      <td>-0.136317</td>\n",
       "      <td>-1.004416</td>\n",
       "      <td>1.022255</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>-0.069778</td>\n",
       "      <td>-1.018322</td>\n",
       "      <td>-0.075816</td>\n",
       "      <td>-0.072802</td>\n",
       "      <td>-0.950192</td>\n",
       "      <td>0.964886</td>\n",
       "      <td>-0.087849</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>-0.297606</td>\n",
       "      <td>-0.061999</td>\n",
       "      <td>-0.100151</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>-0.070444</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.270689</td>\n",
       "      <td>2.269190</td>\n",
       "      <td>2.395081</td>\n",
       "      <td>-1.726292</td>\n",
       "      <td>-1.412031</td>\n",
       "      <td>-0.722256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SK_ID_CURR  NAME_CONTRACT_TYPE_Cash loans  \\\n",
       "0           0   -0.786989                       0.324103   \n",
       "1           1    0.474027                       0.324103   \n",
       "2           2   -0.833941                       0.324103   \n",
       "3           3   -1.475747                      -3.085434   \n",
       "4           4    0.168650                       0.324103   \n",
       "\n",
       "   NAME_CONTRACT_TYPE_Revolving loans  CODE_GENDER_F  CODE_GENDER_M  \\\n",
       "0                           -0.324103       0.722057      -0.722031   \n",
       "1                           -0.324103      -1.384932       1.384982   \n",
       "2                           -0.324103      -1.384932       1.384982   \n",
       "3                            3.085434       0.722057      -0.722031   \n",
       "4                           -0.324103       0.722057      -0.722031   \n",
       "\n",
       "   CODE_GENDER_XNA  FLAG_OWN_CAR_N  FLAG_OWN_CAR_Y  FLAG_OWN_REALTY_N  \\\n",
       "0        -0.004032        0.716969       -0.716969           1.503079   \n",
       "1        -0.004032       -1.394761        1.394761          -0.665301   \n",
       "2        -0.004032        0.716969       -0.716969          -0.665301   \n",
       "3        -0.004032        0.716969       -0.716969          -0.665301   \n",
       "4        -0.004032        0.716969       -0.716969          -0.665301   \n",
       "\n",
       "   FLAG_OWN_REALTY_Y  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0          -1.503079      0.807356         -0.320471    0.654397    -0.120991   \n",
       "1           0.665301     -0.577365          0.041994    1.012907     1.080520   \n",
       "2           0.665301     -0.577365          0.041994   -0.894437    -0.913629   \n",
       "3           0.665301      0.807356         -0.337731   -1.040702    -1.247176   \n",
       "4           0.665301     -0.577365         -0.165129    0.188603    -0.358132   \n",
       "\n",
       "   AMT_GOODS_PRICE  NAME_TYPE_SUITE_missing  NAME_TYPE_SUITE_Group of people  \\\n",
       "0         0.491406                -0.065189                        -0.029781   \n",
       "1         0.978511                -0.065189                        -0.029781   \n",
       "2        -0.945555                -0.065189                        -0.029781   \n",
       "3        -0.969910                -0.065189                        -0.029781   \n",
       "4         0.369630                -0.065189                        -0.029781   \n",
       "\n",
       "   NAME_INCOME_TYPE_Working  NAME_INCOME_TYPE_Commercial associate  \\\n",
       "0                  0.967741                              -0.550989   \n",
       "1                  0.967741                              -0.550989   \n",
       "2                  0.967741                              -0.550989   \n",
       "3                  0.967741                              -0.550989   \n",
       "4                 -1.033335                              -0.550989   \n",
       "\n",
       "   NAME_INCOME_TYPE_State servant  NAME_INCOME_TYPE_Maternity leave  \\\n",
       "0                       -0.275698                         -0.004508   \n",
       "1                       -0.275698                         -0.004508   \n",
       "2                       -0.275698                         -0.004508   \n",
       "3                       -0.275698                         -0.004508   \n",
       "4                       -0.275698                         -0.004508   \n",
       "\n",
       "   NAME_INCOME_TYPE_Businessman  NAME_INCOME_TYPE_Unemployed  \\\n",
       "0                     -0.006049                    -0.008313   \n",
       "1                     -0.006049                    -0.008313   \n",
       "2                     -0.006049                    -0.008313   \n",
       "3                     -0.006049                    -0.008313   \n",
       "4                     -0.006049                    -0.008313   \n",
       "\n",
       "   NAME_FAMILY_STATUS_Married  NAME_FAMILY_STATUS_Civil marriage  \\\n",
       "0                    0.752689                          -0.327059   \n",
       "1                    0.752689                          -0.327059   \n",
       "2                    0.752689                          -0.327059   \n",
       "3                    0.752689                          -0.327059   \n",
       "4                    0.752689                          -0.327059   \n",
       "\n",
       "   NAME_FAMILY_STATUS_Widow  NAME_FAMILY_STATUS_Separated  \\\n",
       "0                 -0.235098                     -0.262467   \n",
       "1                 -0.235098                     -0.262467   \n",
       "2                 -0.235098                     -0.262467   \n",
       "3                 -0.235098                     -0.262467   \n",
       "4                 -0.235098                     -0.262467   \n",
       "\n",
       "   NAME_FAMILY_STATUS_Single / not married  \\\n",
       "0                                -0.417105   \n",
       "1                                -0.417105   \n",
       "2                                -0.417105   \n",
       "3                                -0.417105   \n",
       "4                                -0.417105   \n",
       "\n",
       "   NAME_HOUSING_TYPE_House / apartment  NAME_HOUSING_TYPE_Municipal apartment  \\\n",
       "0                             0.356817                              -0.194440   \n",
       "1                             0.356817                              -0.194440   \n",
       "2                             0.356817                              -0.194440   \n",
       "3                             0.356817                              -0.194440   \n",
       "4                            -2.802553                               5.142968   \n",
       "\n",
       "   NAME_HOUSING_TYPE_With parents  NAME_HOUSING_TYPE_Rented apartment  \\\n",
       "0                       -0.225169                           -0.127727   \n",
       "1                       -0.225169                           -0.127727   \n",
       "2                       -0.225169                           -0.127727   \n",
       "3                       -0.225169                           -0.127727   \n",
       "4                       -0.225169                           -0.127727   \n",
       "\n",
       "   NAME_HOUSING_TYPE_Office apartment  NAME_HOUSING_TYPE_Co-op apartment  \\\n",
       "0                           -0.093034                          -0.060291   \n",
       "1                           -0.093034                          -0.060291   \n",
       "2                           -0.093034                          -0.060291   \n",
       "3                           -0.093034                          -0.060291   \n",
       "4                           -0.093034                          -0.060291   \n",
       "\n",
       "   DAYS_BIRTH  DAYS_EMPLOYED  OWN_CAR_AGE  FLAG_MOBIL  FLAG_EMP_PHONE  \\\n",
       "0    0.968781      -0.477982    -0.146275    0.002016        0.468486   \n",
       "1    0.495804      -0.475964     0.558202    0.002016        0.468486   \n",
       "2   -0.103667      -0.512889    -0.146275    0.002016        0.468486   \n",
       "3    1.746994      -0.461860    -0.146275    0.002016        0.468486   \n",
       "4   -1.567512       2.134588    -0.146275    0.002016       -2.134535   \n",
       "\n",
       "   FLAG_WORK_PHONE  FLAG_CONT_MOBILE  FLAG_PHONE  FLAG_EMAIL  CNT_FAM_MEMBERS  \\\n",
       "0        -0.499666          0.044123   -0.625627   -0.245406         0.930833   \n",
       "1        -0.499666          0.044123   -0.625627   -0.245406        -0.166762   \n",
       "2        -0.499666          0.044123   -0.625627   -0.245406        -0.166762   \n",
       "3        -0.499666          0.044123   -0.625627   -0.245406         0.930833   \n",
       "4        -0.499666          0.044123    1.598398   -0.245406        -0.166762   \n",
       "\n",
       "   REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  HOUR_APPR_PROCESS_START  \\\n",
       "0              1.860366                     1.925206                -1.246885   \n",
       "1              1.860366                     1.925206                 0.898051   \n",
       "2             -0.103735                    -0.063215                -0.021207   \n",
       "3             -0.103735                    -0.063215                 0.591632   \n",
       "4             -0.103735                    -0.063215                 0.285212   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  LIVE_CITY_NOT_WORK_CITY  ORGANIZATION_TYPE  \\\n",
       "0                    -0.206529                 2.136860          -0.674873   \n",
       "1                    -0.206529                -0.467976           0.643345   \n",
       "2                    -0.206529                -0.467976          -0.411544   \n",
       "3                    -0.206529                -0.467976           4.344386   \n",
       "4                    -0.206529                -0.467976          -1.363261   \n",
       "\n",
       "   EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  YEARS_BUILD_AVG  COMMONAREA_AVG  \\\n",
       "0     -2.822123     -2.290413     -0.771477         0.012981       -0.165351   \n",
       "1     -1.676714     -0.205557      1.056196         0.012981       -0.165351   \n",
       "2      0.011026      1.165691      0.604363        -2.887678       -0.483083   \n",
       "3     -2.064219      0.299219      0.112514         0.012981       -0.165351   \n",
       "4      0.011026      0.675098     -1.401479        -1.230159       -0.613434   \n",
       "\n",
       "   ELEVATORS_AVG  ENTRANCES_AVG  FLOORSMIN_AVG  LANDAREA_AVG  \\\n",
       "0      -0.368302      -0.082579      -0.083397     -0.140632   \n",
       "1      -0.368302      -0.082579      -0.083397     -0.140632   \n",
       "2      -0.368302      -1.054522      -1.894904     -0.675604   \n",
       "3      -0.368302      -1.541198      -0.083397     -0.140632   \n",
       "4      -0.368302      -1.054522      -1.442571     -0.639431   \n",
       "\n",
       "   LIVINGAPARTMENTS_AVG  NONLIVINGAPARTMENTS_AVG  NONLIVINGAREA_AVG  \\\n",
       "0             -0.149356                -0.099818          -0.230110   \n",
       "1             -0.149356                -0.099818          -0.230110   \n",
       "2             -1.375431                -0.099818          -0.304587   \n",
       "3             -0.149356                -0.099818          -0.304587   \n",
       "4             -1.313659                -0.099818          -0.304587   \n",
       "\n",
       "   YEARS_BUILD_MODE  COMMONAREA_MODE  ELEVATORS_MODE  ENTRANCES_MODE  \\\n",
       "0          0.026071        -0.168474       -0.355921       -0.050833   \n",
       "1          0.026071        -0.168474       -0.355921       -0.050833   \n",
       "2         -2.839846        -0.443733       -0.355921       -1.016226   \n",
       "3          0.026071        -0.168474       -0.355921       -1.499623   \n",
       "4         -1.201732        -0.576617       -0.355921       -1.016226   \n",
       "\n",
       "   FLOORSMIN_MODE  LIVINGAPARTMENTS_MODE  NONLIVINGAPARTMENTS_MODE  \\\n",
       "0       -0.070287              -0.159456                 -0.094503   \n",
       "1       -0.070287              -0.159456                 -0.094503   \n",
       "2       -1.886891              -1.324711                 -0.094503   \n",
       "3       -0.070287              -0.159456                 -0.094503   \n",
       "4       -1.433285              -1.259485                 -0.094503   \n",
       "\n",
       "   NONLIVINGAREA_MODE  YEARS_BUILD_MEDI  COMMONAREA_MEDI  ELEVATORS_MEDI  \\\n",
       "0           -0.237963          0.013252        -0.166514       -0.365151   \n",
       "1           -0.237963          0.013252        -0.166514       -0.365151   \n",
       "2           -0.260444         -2.880656        -0.477606       -0.365151   \n",
       "3           -0.260444          0.013252        -0.166514       -0.365151   \n",
       "4           -0.260444         -1.226554        -0.607614       -0.365151   \n",
       "\n",
       "   ENTRANCES_MEDI  FLOORSMIN_MEDI  LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  \\\n",
       "0       -0.078828       -0.082291      -0.141441              -0.151557   \n",
       "1       -0.078828       -0.082291      -0.141441              -0.151557   \n",
       "2       -1.048141       -1.887217      -0.671047              -1.369013   \n",
       "3       -1.533500       -0.082291      -0.141441              -0.151557   \n",
       "4       -1.048141       -1.436527      -0.637243              -1.306105   \n",
       "\n",
       "   NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI  FONDKAPREMONT_MODE_missing  \\\n",
       "0                 -0.098784           -0.231454                    0.678940   \n",
       "1                 -0.098784           -0.231454                    0.678940   \n",
       "2                 -0.098784           -0.294935                   -1.472885   \n",
       "3                 -0.098784           -0.294935                    0.678940   \n",
       "4                 -0.098784           -0.294935                   -1.472885   \n",
       "\n",
       "   FONDKAPREMONT_MODE_reg oper spec account  \\\n",
       "0                                 -0.201525   \n",
       "1                                 -0.201525   \n",
       "2                                  4.962173   \n",
       "3                                 -0.201525   \n",
       "4                                 -0.201525   \n",
       "\n",
       "   FONDKAPREMONT_MODE_reg oper account  FONDKAPREMONT_MODE_not specified  \\\n",
       "0                            -0.561708                         -0.136934   \n",
       "1                            -0.561708                         -0.136934   \n",
       "2                            -0.561708                         -0.136934   \n",
       "3                            -0.561708                         -0.136934   \n",
       "4                             1.780285                         -0.136934   \n",
       "\n",
       "   FONDKAPREMONT_MODE_org spec account  HOUSETYPE_MODE_missing  \\\n",
       "0                            -0.136317                0.995603   \n",
       "1                            -0.136317                0.995603   \n",
       "2                            -0.136317               -1.004416   \n",
       "3                            -0.136317               -1.004416   \n",
       "4                            -0.136317               -1.004416   \n",
       "\n",
       "   HOUSETYPE_MODE_block of flats  HOUSETYPE_MODE_terraced house  \\\n",
       "0                      -0.978230                      -0.063015   \n",
       "1                      -0.978230                      -0.063015   \n",
       "2                       1.022255                      -0.063015   \n",
       "3                       1.022255                      -0.063015   \n",
       "4                       1.022255                      -0.063015   \n",
       "\n",
       "   HOUSETYPE_MODE_specific housing  WALLSMATERIAL_MODE_missing  \\\n",
       "0                        -0.069778                    0.982008   \n",
       "1                        -0.069778                    0.982008   \n",
       "2                        -0.069778                   -1.018322   \n",
       "3                        -0.069778                   -1.018322   \n",
       "4                        -0.069778                   -1.018322   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Monolithic  WALLSMATERIAL_MODE_Others  \\\n",
       "0                      -0.075816                  -0.072802   \n",
       "1                      -0.075816                  -0.072802   \n",
       "2                      -0.075816                  -0.072802   \n",
       "3                      -0.075816                  -0.072802   \n",
       "4                      -0.075816                  -0.072802   \n",
       "\n",
       "   EMERGENCYSTATE_MODE_missing  EMERGENCYSTATE_MODE_No  \\\n",
       "0                     1.052419               -1.036392   \n",
       "1                     1.052419               -1.036392   \n",
       "2                    -0.950192                0.964886   \n",
       "3                    -0.950192                0.964886   \n",
       "4                    -0.950192                0.964886   \n",
       "\n",
       "   EMERGENCYSTATE_MODE_Yes  FLAG_DOCUMENT_2  FLAG_DOCUMENT_8  \\\n",
       "0                -0.087849        -0.005703        -0.297606   \n",
       "1                -0.087849        -0.005703        -0.297606   \n",
       "2                -0.087849        -0.005703        -0.297606   \n",
       "3                -0.087849        -0.005703        -0.297606   \n",
       "4                -0.087849        -0.005703        -0.297606   \n",
       "\n",
       "   FLAG_DOCUMENT_11  FLAG_DOCUMENT_16  FLAG_DOCUMENT_21  \\\n",
       "0         -0.061999         -0.100151         -0.018809   \n",
       "1         -0.061999         -0.100151         -0.018809   \n",
       "2         -0.061999         -0.100151         -0.018809   \n",
       "3         -0.061999         -0.100151         -0.018809   \n",
       "4         -0.061999         -0.100151         -0.018809   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                   -0.070444                  -0.058838   \n",
       "1                   -0.070444                  -0.058838   \n",
       "2                   -0.070444                  -0.058838   \n",
       "3                   -0.070444                  -0.058838   \n",
       "4                   -0.070444                  -0.058838   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                   -0.155317                  -0.270689   \n",
       "1                   -0.155317                  -0.270689   \n",
       "2                   -0.155317                   4.422838   \n",
       "3                   -0.155317                  -0.270689   \n",
       "4                   -0.155317                  -0.270689   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \\\n",
       "0                   2.269190                    1.827981   \n",
       "1                  -0.294438                   -0.440420   \n",
       "2                  -0.294438                    1.260881   \n",
       "3                  -0.294438                   -0.440420   \n",
       "4                   2.269190                    2.395081   \n",
       "\n",
       "   NAME_EDUCATION_TYPE_ORDINAL  WEEKDAY_APPR_PROCESS_START_ORDINAL  \\\n",
       "0                    -0.582747                           -0.295316   \n",
       "1                    -0.582747                           -0.295316   \n",
       "2                    -0.582747                            1.938114   \n",
       "3                    -0.582747                            0.263041   \n",
       "4                    -1.726292                           -1.412031   \n",
       "\n",
       "   OCCUPATION_TYPE_MEAN  TARGET  \n",
       "0             -0.659055       0  \n",
       "1             -0.722256       0  \n",
       "2             -0.722256       0  \n",
       "3              1.158337       0  \n",
       "4             -0.722256       0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paths\n",
    "path_data = '../data/application_data.csv'\n",
    "path_data_train_cut = '../data/Processing_data/df_loans_train_cut.csv'\n",
    "path_data_test_cut = '../data/Processing_data/df_loans_test_cut.csv'\n",
    "\n",
    "#insert data\n",
    "df_loans = pd.read_csv(path_data)\n",
    "df_loans_train_cut = pd.read_csv(path_data_train_cut)\n",
    "df_loans_test_cut = pd.read_csv(path_data_test_cut)\n",
    "\n",
    "df_loans_train_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first columns as it is duplicated\n",
    "df_loans_train_cut = df_loans_train_cut.drop('Unnamed: 0',axis=1)\n",
    "df_loans_test_cut = df_loans_test_cut.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61503, 109)\n",
      "(246008, 109)\n"
     ]
    }
   ],
   "source": [
    "# Verify the dimensions of the dataframes after droping out the variables which were not considered relevant\n",
    "print(df_loans_test_cut.shape)\n",
    "print(df_loans_train_cut.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of variables has been reduced from 165 to 109, applying this transformation to both the training and test sets. This approach ensures that the model trains and evaluates with the same features, avoiding biases and guaranteeing consistent evaluation. The reduction of variables enhances model efficiency, reduces the risk of overfitting, and improves its generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SEPARATE TRAIN AND TEST STRATIFIED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our target variable, **TARGET**, exhibits a significant class imbalance, a stratified split of the training and test sets has been implemented. This technique ensures that the class proportions remain consistent across both sets, preventing underrepresentation of the minority class. By preserving the original distribution of the target variable, the model can learn in a more balanced way, leading to improved performance in terms of accuracy and generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features (X) and target variable (y)  \n",
    "X = df_loans_train_cut.drop(columns=['TARGET'])  \n",
    "y = df_loans_train_cut['TARGET']  \n",
    "\n",
    "# Stratified split into training (80%) and testing (20%) sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **UNDERSAMPLING AND OVERSAMPLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the websites provided in class,\n",
    "\n",
    "- [Imbalanced Classification in Python: SMOTE-Tomek Links Method](https://towardsdatascience.com/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc)\n",
    "\n",
    "- [Undersampling Algorithms for Imbalanced Classification](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)\n",
    "\n",
    "we have gathered the following information: \n",
    "\n",
    "One of the popular approaches to solve imbalance dataset problems is either to oversample the minority class or undersample the majority class. This is called:\n",
    "**Undersampling and Oversampling in Imbalanced Classification.** Handling imbalanced data is crucial in classification tasks, and methods like undersampling and oversampling are common approaches to address class imbalance.\n",
    "\n",
    "##### **1- Oversampling**\n",
    "\n",
    "On the other hand, oversampling increases the size of the minority class by generating synthetic data. The most popular method include:\n",
    "\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique):** SMOTE generates synthetic examples of the minority class by interpolating between existing samples. This helps to enhance the representation of the minority class without losing information‚Äã.\n",
    "\n",
    "##### **2- Undersampling**\n",
    "\n",
    "Undersampling involves reducing the size of the majority class by removing some examples. There are several techniques for this:\n",
    "\n",
    "**Near Miss**\n",
    "The Near Miss method selects samples from the majority class based on their proximity to samples in the minority class. There are three versions:\n",
    "- NearMiss-1: Keeps samples from the majority class that are closest to the minority class examples.\n",
    "- NearMiss-2: Selects samples from the majority class that are closest to other majority class examples.\n",
    "- NearMiss-3: Chooses samples that are the farthest from the minority class examples‚Äã\n",
    "\n",
    "**Selective Methods**\n",
    "Some undersampling methods choose examples that best represent the overall data distribution. For example, Condensed Nearest Neighbor preserves crucial samples that define the class boundaries‚Äã\n",
    "\n",
    "**Random Undersumpling** \n",
    "Another straightforward method is randomly removing samples from the majority class. While this approach is fast, it can result in losing critical information‚Äã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Now we will implement them:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1- Oversampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been decided to start with an oversampling approach using **SMOTE**, increasing the minority class to reach **30%** of the majority class, which represents an **18%** increase over its original proportion. This decision was made conservatively, as a higher increase could lead to **overfitting**. This issue arises because generating excessive synthetic data may cause the model to learn artificial patterns specific to the generated data rather than capturing the real relationships present in the original dataset.  \n",
    "\n",
    "This balanced approach aims to improve the model's **predictive performance** without compromising its ability to **generalize to new data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution of the minority class:\n",
      "TARGET\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution after oversampling (SMOTE):\n",
      "TARGET\n",
      "0    0.769232\n",
      "1    0.230768\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "sampling_strategy = 0.3 #minority class should constitute 30% of the overall dataset after oversampling.\n",
    "\n",
    "# Create the SMOTE instance with the specified sampling strategy\n",
    "oversampler = SMOTE(sampling_strategy=sampling_strategy, random_state=seed)\n",
    "\n",
    "# Apply the oversampling to the training data\n",
    "X_train_over, y_train_over = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Verifying Class Distribution Before and After Oversampling (SMOTE) in Python\n",
    "\n",
    "# Original minority class distribution\n",
    "print(\"Original distribution of the minority class:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Distribution after SMOTE oversampling\n",
    "print(\"\\nDistribution after oversampling (SMOTE):\")\n",
    "print(y_train_over.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, it is important to note that **excessive oversampling** may cause the model to learn too much from synthetic instances, which could result in **overfitting**. It is crucial to evaluate how this technique affects model performance by comparing metrics with and without oversampling.\n",
    "\n",
    "**Key metrics to observe to ensure the model is not overfitting:**\n",
    "\n",
    "1. **F1-Score**: Balances **precision and recall**, providing a single metric that reflects both false positives and false negatives.  \n",
    "2. **ROC-AUC Curve**: Evaluates the model's ability to **distinguish between classes**, offering insight into its discriminative power.  \n",
    "3. **Confusion Matrix**: Shows how the model handles **true positives, false positives, true negatives, and false negatives**, making it possible to identify potential biases or errors in classification.  \n",
    "\n",
    "By examining these metrics, you can determine if your oversampling strategy helps in maintaining a balanced and robust model without overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2- Undersampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the significant class imbalance in the dataset (**91.93% for the majority class** and **8.07% for the minority class**), **Random Undersampling** has been chosen as the initial technique. This method **randomly reduces samples from the majority class** to match the minority class, offering a simple and efficient way to address the imbalance.  \n",
    "\n",
    "While more sophisticated methods like **Tomek Links** or **Near Miss** exist‚Äîmethods that aim to **preserve representative examples from the majority class**‚ÄîRandom Undersampling is selected due to its **ease of implementation** and **low computational cost**.  \n",
    "\n",
    "At first, this approach is suitable for evaluating how balancing affects the model **without introducing unnecessary complexity**. Should the need arise, more advanced techniques can be explored in later stages, but **Random Undersampling provides a quick and effective solution during this initial phase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution of the minority class:\n",
      "TARGET\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution after undersampling (Random):\n",
      "TARGET\n",
      "0    0.769231\n",
      "1    0.230769\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Parameters  \n",
    "sampling_strategy = 0.3  # We want the minority class to represent 30% of the total instances  \n",
    "\n",
    "# Create the RandomUnderSampler instance  \n",
    "undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=seed)\n",
    "\n",
    "# Apply undersampling to the training data  \n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verification of the distribution before and after undersampling  \n",
    "print(\"Original distribution of the minority class:\")  \n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribution after undersampling (Random):\")  \n",
    "print(y_train_under.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3- Mix: Undersampling and Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution of the minority class:\n",
      "TARGET\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution after resampling (oversampling + undersampling):\n",
      "TARGET\n",
      "0    0.666667\n",
      "1    0.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define the oversampling and undersampling pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(sampling_strategy=0.5, random_state=seed)),  # Oversampling to increase the minority class\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.5, random_state=seed))  # Undersampling to reduce the majority class\n",
    "])\n",
    "\n",
    "# Display the original class distribution\n",
    "print(\"Original distribution of the minority class:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Apply the transformation to the training data (oversampling + undersampling)\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the distribution after oversampling + undersampling\n",
    "print(\"\\nDistribution after resampling (oversampling + undersampling):\")\n",
    "print(y_train_resampled.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been decided, to apply a 50% increase to the minority class and a 50% reduction to the majority class. This strategy aims to balance the dataset without introducing excessive bias towards the minority class. Oversampling will enhance the representation of the minority class, while undersampling will reduce the influence of the majority class. The chosen balancing approach seeks to improve the model's accuracy without sacrificing relevant information from either class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Evaluation and Selection**\n",
    "\n",
    "This section focuses on evaluating various models with the available datasets to determine the best approach. The goal is to conclude with a well-defined selection of the most suitable model, the optimal dataset for it, and the hyperparameters that maximize performance. The process is structured as follows:\n",
    "\n",
    "1. **Model Comparison**: We will test several models (1 base model + 3 models) with default hyperparameters to identify which offers the best baseline results.\n",
    "2. **Dataset Comparison**: Once a model is chosen, the performance of different datasets will be assessed to determine which one yields the most reliable metrics.  \n",
    "3. **Hyperparameter Tuning**: Finally, a cross-validation-based search will optimize the hyperparameters of the selected model using the chosen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1- Model Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Base model**\n",
    "For our base model we have chosen the **Decision Tree Clasifier**\n",
    "\n",
    "#### **Other models**\n",
    "\n",
    "We decided on evaluating these 4 models: \n",
    "\n",
    "1- Random Forest Classifier  \n",
    "2- XG Boost Classifier   \n",
    "3- Light GBM Classifier  \n",
    "4- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Decision Tree...\n",
      "Classification Report for Decision Tree:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92     45230\n",
      "           1       0.14      0.16      0.15      3972\n",
      "\n",
      "    accuracy                           0.85     49202\n",
      "   macro avg       0.53      0.54      0.53     49202\n",
      "weighted avg       0.86      0.85      0.86     49202\n",
      "\n",
      "Confusion Matrix:\n",
      " [[41204  4026]\n",
      " [ 3338   634]]\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Classification Report for Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.87      0.00      0.01      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.89      0.50      0.48     49202\n",
      "weighted avg       0.92      0.92      0.88     49202\n",
      "\n",
      "Confusion Matrix:\n",
      " [[45228     2]\n",
      " [ 3959    13]]\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Classification Report for XGBoost:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.50      0.03      0.06      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.71      0.51      0.51     49202\n",
      "weighted avg       0.89      0.92      0.89     49202\n",
      "\n",
      "Confusion Matrix:\n",
      " [[45106   124]\n",
      " [ 3847   125]]\n",
      "\n",
      "Evaluating LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7175\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "Classification Report for LightGBM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.61      0.02      0.04      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.76      0.51      0.50     49202\n",
      "weighted avg       0.90      0.92      0.88     49202\n",
      "\n",
      "Confusion Matrix:\n",
      " [[45180    50]\n",
      " [ 3895    77]]\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Classification Report for Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.50      0.01      0.02      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.71      0.50      0.49     49202\n",
      "weighted avg       0.89      0.92      0.88     49202\n",
      "\n",
      "Confusion Matrix:\n",
      " [[45187    43]\n",
      " [ 3929    43]]\n"
     ]
    }
   ],
   "source": [
    "# List of models to test  \n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=seed),\n",
    "    'Random Forest': RandomForestClassifier(random_state=seed),\n",
    "    'XGBoost': XGBClassifier(random_state=seed, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(random_state=seed),\n",
    "    'Logistic Regression': LogisticRegression(random_state=seed)\n",
    "}\n",
    "\n",
    "# Evaluate each model  \n",
    "results = {}  \n",
    "for name, model in models.items():  \n",
    "    print(f\"\\nEvaluating {name}...\")  \n",
    "    model.fit(X_train, y_train)  \n",
    "    y_pred = model.predict(X_test)  \n",
    "    print(f\"Classification Report for {name}:\\n\")  \n",
    "    print(classification_report(y_test, y_pred))  \n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))  \n",
    "    results[name] = classification_report(y_test, y_pred, output_dict=True)  # Save results for analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize all the metrics all in once, we created a table for better analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Comparative Model Table \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision (1)</th>\n",
       "      <th>Recall (1)</th>\n",
       "      <th>F1-Score (1)</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Detected Cases (1)</th>\n",
       "      <th>Total Class 1</th>\n",
       "      <th>Percentage Detected (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>634</td>\n",
       "      <td>3972</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>13</td>\n",
       "      <td>3972</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.51</td>\n",
       "      <td>125</td>\n",
       "      <td>3972</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.51</td>\n",
       "      <td>77</td>\n",
       "      <td>3972</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.50</td>\n",
       "      <td>43</td>\n",
       "      <td>3972</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision (1)  Recall (1)  F1-Score (1)  Macro F1  \\\n",
       "0        Decision Tree           0.14        0.16          0.15      0.53   \n",
       "1        Random Forest           0.87        0.00          0.01      0.48   \n",
       "2              XGBoost           0.50        0.03          0.06      0.51   \n",
       "3             LightGBM           0.61        0.02          0.04      0.50   \n",
       "4  Logistic Regression           0.50        0.01          0.02      0.49   \n",
       "\n",
       "   Macro Precision  Macro Recall  Detected Cases (1)  Total Class 1  \\\n",
       "0             0.53          0.54                 634           3972   \n",
       "1             0.89          0.50                  13           3972   \n",
       "2             0.71          0.51                 125           3972   \n",
       "3             0.76          0.51                  77           3972   \n",
       "4             0.71          0.50                  43           3972   \n",
       "\n",
       "   Percentage Detected (1)  \n",
       "0                    15.96  \n",
       "1                     0.33  \n",
       "2                     3.15  \n",
       "3                     1.94  \n",
       "4                     1.08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data from the models' classification reports  \n",
    "results = {  \n",
    "    'Model': ['Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM', 'Logistic Regression'],  \n",
    "    'Precision (1)': [0.14, 0.87, 0.50, 0.61, 0.50],  \n",
    "    'Recall (1)': [0.16, 0.00, 0.03, 0.02, 0.01],  \n",
    "    'F1-Score (1)': [0.15, 0.01, 0.06, 0.04, 0.02],  \n",
    "    'Macro F1': [0.53, 0.48, 0.51, 0.50, 0.49],  \n",
    "    'Macro Precision': [0.53, 0.89, 0.71, 0.76, 0.71],  \n",
    "    'Macro Recall': [0.54, 0.50, 0.51, 0.51, 0.50],  \n",
    "    'Detected Cases (1)': [634, 13, 125, 77, 43],  \n",
    "    'Total Class 1': [3972, 3972, 3972, 3972, 3972],  # Total real cases of Class 1  \n",
    "    'Percentage Detected (1)': [634 / 3972 * 100, 13 / 3972 * 100, 125 / 3972 * 100, 77 / 3972 * 100, 43 / 3972 * 100]  \n",
    "}  \n",
    "\n",
    "# Create DataFrame with the results  \n",
    "df_results = pd.DataFrame(results)  \n",
    "\n",
    "# Round percentages and metrics for better visualization  \n",
    "df_results['Percentage Detected (1)'] = df_results['Percentage Detected (1)'].round(2)  \n",
    "df_results.iloc[:, 1:-2] = df_results.iloc[:, 1:-2].round(2)  \n",
    "\n",
    "# Display the comparative table  \n",
    "print(\"\\n Comparative Model Table \\n\")  \n",
    "display(df_results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**\n",
    "\n",
    "In this analysis, class 1 (clients who will not repay the loan) is the most relevant, as the goal is to identify defaults. Regarding class detection, the **Decision Tree** model detects 634 out of 3972 cases, with a recall of 0.16, while **Random Forest** almost ignores class 1, detecting only 13 cases (0.33%). **XGBoost** and **LightGBM** also have low recall values of 0.03 and 0.02, respectively, but both models offer multiple hyperparameters that can be adjusted to improve minority class detection. These adjustments include modifying **scale_pos_weight**, reducing **learning_rate**, increasing **n_estimators**, and tweaking **max_depth** and **min_child_weight**.\n",
    "\n",
    "In terms of overall metrics, the **Decision Tree** model has a **macro F1** score of 0.53, which is higher than **Random Forest** (0.48) and comparable to **XGBoost** (0.51) and **LightGBM** (0.50). **Random Forest** has high precision (0.89), but its low recall (0.50) makes it less effective for this problem. **XGBoost** and **LightGBM** offer a better balance between **precision** and **recall**, being more consistent. **XGBoost** is highly stable and includes regularization to prevent overfitting, while **LightGBM** is faster and could improve further with proper adjustments.\n",
    "\n",
    "**Next Septs:** **XGBoost** and **LightGBM** will be evaluated more thoroughly, focusing on tuning their hyperparameters to maximize performance, particularly in detecting class 1. Adjusting key parameters such as **scale_pos_weight**, **learning_rate**, and **n_estimators** will help optimize both models and improve the ability to identify defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2- Dataset Comparison: XGBoost vs LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure train and test DataFrames have aligned columns \n",
    "X_train, X_test = X_train.align(X_test, join='inner', axis=1)  \n",
    "X_train_over, X_test = X_train_over.align(X_test, join='inner', axis=1)  \n",
    "X_train_under, X_test = X_train_under.align(X_test, join='inner', axis=1)  \n",
    "X_train_resampled, X_test = X_train_resampled.align(X_test, join='inner', axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating XGBoost...\n",
      "\n",
      "\n",
      "Evaluating with dataset: Original...\n",
      "Classification report for dataset Original:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.50      0.03      0.06      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.71      0.51      0.51     49202\n",
      "weighted avg       0.89      0.92      0.89     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[45106   124]\n",
      " [ 3847   125]]\n",
      "\n",
      "Evaluating with dataset: Oversampled...\n",
      "Classification report for dataset Oversampled:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.52      0.04      0.07      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.72      0.52      0.51     49202\n",
      "weighted avg       0.89      0.92      0.89     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[45100   130]\n",
      " [ 3832   140]]\n",
      "\n",
      "Evaluating with dataset: Undersampled...\n",
      "Classification report for dataset Undersampled:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     45230\n",
      "           1       0.28      0.25      0.26      3972\n",
      "\n",
      "    accuracy                           0.89     49202\n",
      "   macro avg       0.61      0.60      0.60     49202\n",
      "weighted avg       0.88      0.89      0.88     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[42652  2578]\n",
      " [ 2972  1000]]\n",
      "\n",
      "Evaluating with dataset: Mix (Over + Under)...\n",
      "Classification report for dataset Mix (Over + Under):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.50      0.04      0.07      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.71      0.52      0.51     49202\n",
      "weighted avg       0.89      0.92      0.89     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[45076   154]\n",
      " [ 3819   153]]\n",
      "\n",
      "Evaluating LightGBM...\n",
      "\n",
      "\n",
      "Evaluating with dataset: Original...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7175\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "Classification report for dataset Original:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.61      0.02      0.04      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.76      0.51      0.50     49202\n",
      "weighted avg       0.90      0.92      0.88     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[45180    50]\n",
      " [ 3895    77]]\n",
      "\n",
      "Evaluating with dataset: Oversampled...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 54275, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19934\n",
      "[LightGBM] [Info] Number of data points in the train set: 235193, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.230768 -> initscore=-1.203980\n",
      "[LightGBM] [Info] Start training from score -1.203980\n",
      "Classification report for dataset Oversampled:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.56      0.02      0.04      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.74      0.51      0.50     49202\n",
      "weighted avg       0.89      0.92      0.88     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[45166    64]\n",
      " [ 3889    83]]\n",
      "\n",
      "Evaluating with dataset: Undersampled...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 52960\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6786\n",
      "[LightGBM] [Info] Number of data points in the train set: 68848, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.230769 -> initscore=-1.203973\n",
      "[LightGBM] [Info] Start training from score -1.203973\n",
      "Classification report for dataset Undersampled:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94     45230\n",
      "           1       0.31      0.23      0.27      3972\n",
      "\n",
      "    accuracy                           0.90     49202\n",
      "   macro avg       0.62      0.59      0.60     49202\n",
      "weighted avg       0.88      0.90      0.89     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[43197  2033]\n",
      " [ 3053   919]]\n",
      "\n",
      "Evaluating with dataset: Mix (Over + Under)...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 90459, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20875\n",
      "[LightGBM] [Info] Number of data points in the train set: 271377, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333333 -> initscore=-0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "Classification report for dataset Mix (Over + Under):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     45230\n",
      "           1       0.60      0.02      0.04      3972\n",
      "\n",
      "    accuracy                           0.92     49202\n",
      "   macro avg       0.76      0.51      0.50     49202\n",
      "weighted avg       0.90      0.92      0.88     49202\n",
      "\n",
      "Confusion matrix:\n",
      " [[45179    51]\n",
      " [ 3894    78]]\n"
     ]
    }
   ],
   "source": [
    "# Define the datasets  \n",
    "datasets = {\n",
    "    'Original': (X_train, y_train),\n",
    "    'Oversampled': (X_train_over, y_train_over),\n",
    "    'Undersampled': (X_train_under, y_train_under),\n",
    "    'Mix (Over + Under)': (X_train_resampled, y_train_resampled)\n",
    "}\n",
    "\n",
    "# Define the models  \n",
    "models = {\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=seed),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=seed)\n",
    "}\n",
    "\n",
    "# Evaluate both models on each dataset  \n",
    "model_results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\\n\")\n",
    "    \n",
    "    # Evaluate the model on each dataset  \n",
    "    dataset_results = {}\n",
    "    for dataset_name, (X, y) in datasets.items():\n",
    "        print(f\"\\nEvaluating with dataset: {dataset_name}...\")\n",
    "\n",
    "        # Train the model on the corresponding training data  \n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Make predictions on the test set  \n",
    "        y_pred = model.predict(X_test)  # Use X_test for all cases\n",
    "\n",
    "        # Print the evaluation results  \n",
    "        print(f\"Classification report for dataset {dataset_name}:\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # Save the results for analysis  \n",
    "        dataset_results[dataset_name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Save the results for the model  \n",
    "    model_results[model_name] = dataset_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**\n",
    "\n",
    "The XGBoost model struggles to detect class 1 (customers who will not repay the loan) across all datasets, with a consistently low recall.\n",
    "\n",
    "- Original Dataset: Recall for class 1 is only 3%, with an f1-score of 0.06. Precision for class 0 is high (92%), but class 1 detection is poor.\n",
    "\n",
    "- Oversampled Dataset: Recall improves slightly to 4%, with an f1-score of 0.07, but remains low.\n",
    "\n",
    "- Undersampled Dataset: Shows some improvement with a 25-30% recall, but still limited detection for class 1 customers.\n",
    "\n",
    "- Mix Dataset: Similar to the original and oversampled datasets, with poor recall for class 1 detection and no significant gains in f1-score.\n",
    "\n",
    "**LightGBM**\n",
    "\n",
    "- Original Dataset: The LightGBM model shows low recall performance for class 1 (customers who will not repay the loan), with only 2% recall. Despite achieving a high precision of 92% for class 0 (no repayment difficulties), it struggles to identify customers with repayment issues, reflected in an f1-score of 0.04.\n",
    "\n",
    "- In the oversampled dataset, recall remains low at 2%, while precision stays high (92%). The f1-score remains 0.04, showing only a slight improvement compared to the original dataset but still failing to detect class 1 customers effectively.\n",
    "\n",
    "- Undersampled Dataset: The model's recall improves slightly to 23%, with a f1-score of 0.27. Precision remains high (93% for class 0), but recall for class 1 is still limited, indicating a more balanced detection of repayment issues but with considerable room for improvement.\n",
    "\n",
    "- Mix Dataset: Recall for class 1 remains very low at 2%, similar to the oversampled dataset. Although overall accuracy is 92%, the model still struggles to detect customers who will default on the loan, maintaining the same f1-score of 0.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model of choice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although both models perform similarly in detecting class 1, XGBoost offers greater flexibility in hyperparameter optimization, such as `scale_pos_weight` and `learning_rate`, which could significantly enhance its performance. Additionally, XGBoost is more robust against overfitting due to its built-in regularization. Despite the low recall, its tuning capabilities make it a more promising choice in this case.\n",
    "\n",
    "Specifically, the combination **Mix + XGBoost** will be chosen.\n",
    "\n",
    "- Recall (class 1) = 0.02  \n",
    "- F1-Score (class 1) = 0.04*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODEL: XGBoost | Mix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, randomized search was employed to optimize the hyperparameters of the XGBoost model, with the goal of accurately predicting loan defaults. Given the large dataset (246,008 rows and 109 columns), this technique is computationally more efficient than exhaustive grid search, allowing for exploration of a wider parameter space in less time. Furthermore, numerous studies have shown that randomized search often finds high-quality solutions more quickly.\n",
    "\n",
    "The choice of evaluation metric is crucial in this context. While AUC is a popular metric, in this specific case where the detection of defaults is a priority, the **F2 score was selected**. This metric places a higher weight on recall, ensuring the identification of most default cases, which is essential for minimizing financial risk.\n",
    "\n",
    "To obtain a more comprehensive evaluation of the model, multiple metrics were considered, including AUC, F1-score, precision, and recall. However, the F2 score was the primary metric due to its alignment with the business objective: reducing the risk of default.\n",
    "\n",
    "The choice of XGBoost is justified by its ability to handle class imbalances, such as the one present in this dataset where defaults are a minority class. Additionally, the flexibility in configuring its hyperparameters allows the model to be tailored to the specific characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      " Best hyperparameters found: {'subsample': 0.8, 'scale_pos_weight': 10, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.8}\n",
      "\n",
      " Best F2 score obtained: 0.7603278419264772\n",
      "\n",
      " Results for all parameter combinations:\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Define custom F2-score (prioritizes recall)\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2, pos_label=1)  # Positive class (default) is 1\n",
    "\n",
    "# 3. Define base XGBoost model\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(random_state=seed, eval_metric='logloss', tree_method='hist')\n",
    "\n",
    "# 4. Hyperparameter search space\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],  # Number of trees\n",
    "    'max_depth': [3, 4],        # Maximum depth of trees\n",
    "    'learning_rate': [0.05, 0.1],  # Learning rate\n",
    "    'colsample_bytree': [0.8],  # Subsample of features\n",
    "    'subsample': [0.8],        # Subsample of rows\n",
    "    'gamma': [0, 1],            # Minimum loss reduction\n",
    "    'reg_alpha': [0.1],         # L1 regularization\n",
    "    'reg_lambda': [1],          # L2 regularization\n",
    "    'scale_pos_weight': [10, 20, 50]  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# 5. Randomized search for hyperparameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring=f2_scorer,\n",
    "    refit='f2',  # Refit the best estimator using the F2 score\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6. Fit the model on the undersampled training data\n",
    "xgb_random.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 7. Print best hyperparameters\n",
    "print(\"\\n Best hyperparameters found:\", xgb_random.best_params_)\n",
    "\n",
    "# 8. Print best F2 score\n",
    "print(\"\\n Best F2 score obtained:\", xgb_random.best_score_)\n",
    "\n",
    "# 9. Print all results\n",
    "print(\"\\n Results for all parameter combinations:\")\n",
    "results = xgb_random.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**\n",
    "\n",
    "The primary objective of this model was to maximize the detection of loan defaults, prioritizing the identification of customers with a higher risk of default. This strategy is aligned with a context where the goal is not to expand the customer base but to reduce the risk associated with granting loans. To achieve this, the model was optimized to maximize the F2-score (0.76), as this metric places a greater emphasis on recall, which is crucial for detecting the minority class (defaults).\n",
    "\n",
    "The model achieved this F2-score with the following optimal hyperparameters: a maximum depth of 3, 100 trees, a learning rate of 0.05, and a scale_pos_weight of 20, reinforcing the importance of the default class. L1 and L2 regularization techniques were applied, along with fractional sampling of rows and columns (subsample=0.8 and colsample_bytree=0.8), to control overfitting and improve generalization.\n",
    "\n",
    "The model's performance is reflected in key metrics:\n",
    "\n",
    "- Recall (0.9912): The model correctly identifies 99.12% of defaults, fulfilling the objective of reducing risk.\n",
    "- Precision (0.6745): Precision is acceptable in this context, where it is preferable to have false positives rather than miss actual defaults.\n",
    "- ROC-AUC (0.8998): The ability to discriminate between high-risk and low-risk customers is outstanding.\n",
    "\n",
    "An analysis of the top 5 configurations shows that adjusting the scale_pos_weight was crucial, with values between 10, 20, and 50 standing out among the best combinations. The winning configuration achieved a balance between precision, recall, and F2, prioritizing the capture of defaults. Although other combinations achieved a higher recall, their precision was lower, making the final model a better compromise.\n",
    "\n",
    "In conclusion, the model is aligned with the objective of reducing loan risk by focusing on identifying customers with a higher probability of default. The high sensitivity (recall) ensures that almost all defaults are detected, even at the cost of accepting some false positives. This configuration is suitable for financial environments where risk reduction is a strategic priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Performance (F2-Optimized): Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 11. Prediction with the best model\n",
    "best_model = xgb_random.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 12. Generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 13. Visualize confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix: F2-Optimized XGBoost Model for Predicting Loan Defaults')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusions**\n",
    "\n",
    "The high number of false positives (FP) in this model is justified by the prioritized focus on minimizing the risk of loan defaults. **The primary objective has been to conservatively identify cases that could result in defaults, even at the cost of misclassifying some repaid loans as defaults.** This approach is essential in a context where the economic risk of defaults is significant and it is preferable to err on the side of caution by rejecting some loans, rather than risk losing money on unpaid loans.\n",
    "\n",
    "Additionally, given the class imbalance, the model has been designed to prioritize recall (sensitivity), ensuring that most defaults are captured, even if this increases false positives. This approach avoids approving loans that may subsequently default, which incurs a much higher cost than false positives.\n",
    "\n",
    "In conclusion, the model sacrifices some precision to guarantee financial security, preventing greater losses from loan defaults and, therefore, reducing the overall risk of the portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **END OF NOTEBOOK 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Save the resampled training set (X_train_resampled + y_train_resampled) as CSV\n",
    "pd.concat([X_train_resampled, y_train_resampled], axis=1).to_csv('../data/Processing_data/df_loans_train_resampled.csv', index=False)\n",
    "\n",
    "\n",
    "# Crear la carpeta 'models' si no existe\n",
    "os.makedirs('/Users/bris2/Desktop/GemaMaster/Aprendizaje/Practica1_EDA/models', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "dump(xgb_random.best_estimator_, '/Users/bris2/Desktop/GemaMaster/Aprendizaje/Practica1_EDA/models/best_estimator.joblib')\n",
    "\n",
    "# 3. Save the best parameters found in a CSV file\n",
    "best_params_df = pd.DataFrame([xgb_random.best_params_])\n",
    "best_params_df.to_csv('../data/Processing_data/best_xgb_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
